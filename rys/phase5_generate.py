#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 5: Script Generation (v2.13)
Generates bash scripts for jobs based on the I/O plan in TOON format.
Updated: Renamed REQUEST to Job (v2.13)
"""

import sys
import os
import json
import argparse
from phase_utils import invoke_coder, prepare_coder_prompt, append_result_display

# pylint: disable=useless-return

# Add project root to path for utilities
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
if SCRIPT_DIR not in sys.path:
    sys.path.append(SCRIPT_DIR)


def process_single_job(job, args, llm_config):
    """Processes one job: codes tasks and writes the script."""
    job_id = job["job_id"]
    skill = job["skill"]

    print(f"Handling {job_id} ({skill})...")
    script_lines = ["#!/bin/bash", "# Generated by RYS Phase 5", ""]
    final_binding = None

    for task in job["tasks"]:
        print(f"  Coding {task['id']}...")
        prompt = prepare_coder_prompt(task)
        snippet = invoke_coder(SCRIPT_DIR, prompt, skill, llm_config)
        script_lines.append(f"# --- {task['id']}: {task['title']} ---")
        script_lines.append(snippet)
        script_lines.append("")
        if task.get('output', {}).get('binding'):
            final_binding = task['output']['binding']

    append_result_display(script_lines, job_id, final_binding)

    cache_dir = os.environ.get("RYS_CACHE_DIR", "./tmp")
    script_fn = f".rys.{args.uuid}.{job_id}.sh"
    script_path = os.path.abspath(os.path.join(cache_dir, script_fn))

    with open(script_path, "w", encoding="utf-8") as f_out:
        f_out.write("\n".join(script_lines))
    os.chmod(script_path, 0o755)

    return {"job_id": job_id, "path": script_path, "skill": skill}


def load_existing_scripts(out_json):
    """Loads previously generated scripts from output JSON."""
    existing = {}
    if os.path.exists(out_json):
        try:
            with open(out_json, 'r', encoding='utf-8') as f_in:
                old_data = json.load(f_in)
                for script in old_data.get("generated_scripts", []):
                    existing[script["job_id"]] = script
        except (json.JSONDecodeError, KeyError, OSError):
            pass
    return existing


def main():
    """Main execution for Phase 5 script generation."""
    parser = argparse.ArgumentParser()
    parser.add_argument("--in-json", required=True)
    parser.add_argument("--out-json", required=True)
    parser.add_argument("--uuid", required=True)
    parser.add_argument("--host", default="localhost")
    parser.add_argument("--port")
    parser.add_argument("--model", default="gemma3n:e4b")
    parser.add_argument("--job", "--request", help="Specific Job ID to regenerate")
    args = parser.parse_args()

    if not os.path.exists(args.in_json):
        sys.exit(1)

    with open(args.in_json, 'r', encoding='utf-8') as f_in:
        data = json.load(f_in)

    llm_config = {"host": args.host, "port": args.port, "model": args.model}
    existing_scripts = load_existing_scripts(args.out_json)
    script_list = []

    print(f">>> Phase 5: Generating Scripts (Session: {args.uuid})")

    for job in data.get("integrated_jobs", []):
        job_id = job["job_id"]
        if job["skill"] == "IDONTKNOW":
            continue

        if args.job and job_id != args.job:
            if job_id in existing_scripts:
                print(f"Skipping {job_id} (Keeping existing script)")
                script_list.append(existing_scripts[job_id])
            continue

        script_list.append(process_single_job(job, args, llm_config))

    data["generated_scripts"] = script_list
    with open(args.out_json, "w", encoding="utf-8") as f_out:
        json.dump(data, f_out, indent=2)

    print(f"Phase 5 Complete. {len(script_list)} scripts tracked.\n")
    return


if __name__ == "__main__":
    main()
